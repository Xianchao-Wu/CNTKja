{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "original_source": [
     "# CNTK 303: Deep Structured Semantic Modeling with LSTM Networks\n",
     "\n",
     "DSSM stands for Deep Structured Semantic Model, or more general, Deep Semantic Similarity Model. DSSM, developed by the MSR Deep Learning Technology Center(DLTC), is a deep neural network (DNN) modeling technique for representing text strings (sentences, queries, predicates, entity mentions, etc.) in a continuous semantic space and modeling semantic similarity between two text strings (e.g., Sent2Vec). DSSM has wide applications including information retrieval and web search ranking ([Huang et al. 2013](https://www.microsoft.com/en-us/research/publication/learning-deep-structured-semantic-models-for-web-search-using-clickthrough-data/); [Shen et al. 2014a](https://www.microsoft.com/en-us/research/publication/learning-semantic-representations-using-convolutional-neural-networks-for-web-search/),[2014b](https://www.microsoft.com/en-us/research/publication/a-latent-semantic-model-with-convolutional-pooling-structure-for-information-retrieval/)), ad selection/relevance, contextual entity search and interestingness tasks ([Gao et al. 2014a](https://www.microsoft.com/en-us/research/publication/modeling-interestingness-with-deep-neural-networks/), question answering ([Yih et al., 2014](https://www.microsoft.com/en-us/research/publication/semantic-parsing-for-single-relation-question-answering/)), image captioning ([Fang et al., 2014](https://arxiv.org/abs/1411.4952)), and machine translation ([Gao et al., 2014b](https://www.microsoft.com/en-us/research/publication/learning-continuous-phrase-representations-for-translation-modeling/)) etc. \n",
     "\n",
     "DSSM can be used to develop latent semantic models that project entities of different types (e.g., queries and documents) into a common low-dimensional semantic space for a variety of machine learning tasks such as ranking and classification. For example, in web search ranking, the relevance of a document given a query can be readily computed as the distance between them in that space. With the latest GPUs from Nvidia, we can train our models on billions of words. Readers that are interested in deep learning for text processing may refer to the tutorial by [He et al., 2014](https://www.microsoft.com/en-us/research/publication/deep-learning-for-natural-language-processing-theory-and-practice-tutorial/).\n",
     "We released the predictors and trained model files of the DSSM (also a.k.a. Sent2Vec).\n",
     "\n",
     "## Goal\n",
     "\n",
     "To develop mechanism such that given a pair of documents say a query and a set of web page documents, the model would map the inputs to a pair of feature vectors in a continuous, low dimensional space where one could compare the semantic similarity between the text strings using the cosine similarity between their vectors in that space.  \n",
     "\n",
     "![](http://kubicode.me/img/Study-With-Deep-Structured-Semantic-Model/dssm_arch.png)\n",
     "\n",
     "In the figure above one can see how given a query ($Q$) and set of documents ($D_1, D_2, \\ldots, D_n$), one can generate latent representation a.k.a. semantic features, which can then be used to generate pairwise distance metric. The metric evaluated can be used for ranking."
    ]
   },
   "source": [
    "_unchecked_\n",
    "\n",
    "# CNTK 303: LSTM 网络的深层结构语义建模\n",
    "\n",
    "DSSM 代表了深层的结构语义模型, 或者更一般的、深层语义相似性模型。DSSM, 由 MSR 深学习技术中心 (DLTC) 开发, 是一种深神经网络 (DNN) 建模技术, 用于表示连续语义空间中的文本字符串 (句子、查询、谓词、实体提及等), 并建模语义两个文本字符串 (例如, Sent2Vec) 的相似性。DSSM 有广泛的应用, 包括信息检索和网页搜索排名 ([黄 et al. 2013](https://www.microsoft.com/en-us/research/publication/learning-deep-structured-semantic-models-for-web-search-using-clickthrough-data/);[沈 et al. 2014a](https://www.microsoft.com/en-us/research/publication/learning-semantic-representations-using-convolutional-neural-networks-for-web-search/),[2014b](https://www.microsoft.com/en-us/research/publication/a-latent-semantic-model-with-convolutional-pooling-structure-for-information-retrieval/)), ad 选择/相关性, 上下文实体搜索和兴趣任务 ([高 et al. 2014a](https://www.microsoft.com/en-us/research/publication/modeling-interestingness-with-deep-neural-networks/), 问答 ([亿等 al., 2014](https://www.microsoft.com/en-us/research/publication/semantic-parsing-for-single-relation-question-answering/)), 图像字幕 ([方 et al., 2014](https://arxiv.org/abs/1411.4952)), 和机器翻译 ([高 et al., 2014b](https://www.microsoft.com/en-us/research/publication/learning-continuous-phrase-representations-for-translation-modeling/)) 等。\n",
    "\n",
    "DSSM 可用于开发不同类型的项目实体 (例如, 查询和文档) 的潜在语义模型, 以用于各种机器学习任务 (如排序和分类) 的公共低维语义空间。例如, 在 web 搜索排名中, 给定查询的文档的相关性可以很容易地计算为在该空间中它们之间的距离。从 Nvidia 的最新 gpu, 我们可以训练我们的模型上亿个字。对文本处理的深入学习感兴趣的读者可以通过[他等 al., 2014](https://www.microsoft.com/en-us/research/publication/deep-learning-for-natural-language-processing-theory-and-practice-tutorial/)来参考教程。\n",
    "我们发布了 DSSM (又名 Sent2Vec) 的预测因子和训练有素的模型文件。\n",
    "\n",
    "## 目标\n",
    "\n",
    "为了开发这样的机制, 如果给定了一对文档的查询和一组网页文档, 该模型将把输入映射到一个连续的、低维空间中的一对特征向量, 其中一个可以比较文本字符串之间的语义相似性在该空间中使用它们的向量之间的余弦相似性。\n",
    "\n",
    "![](http://kubicode.me/img/Study-With-Deep-Structured-Semantic-Model/dssm_arch.png)\n",
    "\n",
    "在上面的图中, 你可以看到如何给定一个查询 ($ Q $) 和一组文档 ($ D_1, D_2, \\ldots, D_n $), 你可以生成潜在的表示, 又称语义特征, 然后可以用来生成成对距离度量。度量值可用于排序。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "original_source": [
     "In the picture above, one can see that the query and the document are each mapped to a term vector. While a [bag of word](https://en.wikipedia.org/wiki/Bag-of-words_model) based modeling is a first step one takes while building NLP models, they are limited in their ability to capture relative positions amongst words. Convolution based, or recurrence based models perform better due to their inherent ability to leverage the positions of words. In this tutorial, we will use a simple illustrative model using LSTM to encode the term vector following the work done by [Palangi et. al.](https://www.microsoft.com/en-us/research/wp-content/uploads/2017/02/LSTM_DSSM_IEEE_TASLP.pdf). \n",
     "\n",
     "In this tutorial, we show you how to build such a network.  We use a small sample from the Question-Answering corpus. Additionally we will use a recurrent network to develop the semantic model as it allows to inherently incorporate the positional information with the word tokens. \n",
     "\n",
     "**Note**: The data set is very small and the emphasis of this tutorial is in showing how to create an end-to-end modeling workflow for the DSSM network and not so much on the specific numerical performance we are able to get on this small data set. "
    ]
   },
   "source": [
    "_unchecked_\n",
    "\n",
    "在上面的图片中, 人们可以看到查询和文档都映射到一个术语向量。虽然基于[word 包](https://en.wikipedia.org/wiki/Bag-of-words_model)的建模是在构建 NLP 模型时需要的第一步, 但它们在单词之间捕获相对位置的能力受到限制。基于卷积的或基于重复的模型的性能更好, 因为它们具有利用单词位置的内在能力。在本教程中, 我们将使用一个简单的说明模型, 使用 LSTM 在[Palangi et. al.](https://www.microsoft.com/en-us/research/wp-content/uploads/2017/02/LSTM_DSSM_IEEE_TASLP.pdf)完成的工作之后对术语向量进行编码。\n",
    "\n",
    "在本教程中, 我们将向您展示如何构建这样一个网络。 我们使用问答语料库中的一个小样本。此外, 我们将使用一个经常性的网络来开发语义模型, 因为它允许将位置信息与单词标记结合在一起。\n",
    "\n",
    "**注意**: 数据集非常小, 本教程的重点是演示如何为 DSSM 网络创建最终的建模工作流, 而不是我们能够在这个小数据集上获得的特定数值性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the relevant libraries\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "from __future__ import print_function # Use a function definition from future version (say 3.x from 2.7 interpreter)\n",
    "\n",
    "import cntk as C\n",
    "import cntk.tests.test_utils\n",
    "cntk.tests.test_utils.set_device_from_pytest_env() # (only needed for our build system)\n",
    "C.cntk_py.set_fixed_random_seed(1) # fix a random seed for CNTK components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "original_source": [
     "## Data Preparation\n",
     "\n",
     "### Download\n",
     "\n",
     "We use a sampling of the Question Answering data set for illustrating how to model DSSM networks. The data set consists of pair of sentences with [Questions and Answers](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ACL15-STAGG.pdf). In this tutorial, we have preprocessed the data into two parts:\n",
     "- Vocabulary files: 1 file each for question and answers. There are 1204 and 1019 words in the question and answers vocabulary, respectively.\n",
     "- QA files: 1 file each for training and validation data (hold-out) where each of the files are converted in the [CTF format](https://cntk.ai/pythondocs/CNTK_202_Language_Understanding.html). The training and validation files have 3500 and 409 sentence pairs respectively.\n",
     "\n",
     "Note: a small portion of the original data was provided by the author of the paper for creating an exemplar network for illustration purposes."
    ]
   },
   "source": [
    "_unchecked_\n",
    "\n",
    "## 数据准备\n",
    "\n",
    "### 下载\n",
    "\n",
    "我们使用问答数据集的抽样来说明如何建模 DSSM 网络。数据集由一对具有[问题和答案](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/ACL15-STAGG.pdf)的句子组成。在本教程中, 我们将数据预处理为两个部分:-词汇文件: 1 文件每一个问题和答案。有1204和1019单词的问题和答案的词汇, 分别。\n",
    "-QA 文件: 1 文件每个文件的培训和验证数据 (保持), 其中每一个档案转换为[周大福格式](https://cntk.ai/pythondocs/CNTK_202_Language_Understanding.html)。训练和验证文件分别有3500和409句对。\n",
    "\n",
    "注: 本文作者提供了一小部分原始数据, 用于创建范例网络以供说明之用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download: vocab_A.wl\n",
      "http://www.cntk.ai/jup/dat/DSSM/vocab_A.wl.csv\n",
      "Download completed\n",
      "Starting download: train.pair.tok.ctf\n",
      "http://www.cntk.ai/jup/dat/DSSM/train.pair.tok.ctf.csv\n",
      "Download completed\n",
      "Starting download: valid.pair.tok.ctf\n",
      "http://www.cntk.ai/jup/dat/DSSM/valid.pair.tok.ctf.csv\n",
      "Download completed\n",
      "Starting download: vocab_Q.wl\n",
      "http://www.cntk.ai/jup/dat/DSSM/vocab_Q.wl.csv\n",
      "Download completed\n"
     ]
    }
   ],
   "source": [
    "location = os.path.normpath('data/DSSM')\n",
    "data = {\n",
    "  'train': { 'file': 'train.pair.tok.ctf' },\n",
    "  'val':{ 'file': 'valid.pair.tok.ctf' },\n",
    "  'query': { 'file': 'vocab_Q.wl' },\n",
    "  'answer': { 'file': 'vocab_A.wl' }\n",
    "}\n",
    "\n",
    "import requests\n",
    "\n",
    "def download(url, filename):\n",
    "    \"\"\" utility function to download a file \"\"\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(filename, \"wb\") as handle:\n",
    "        for data in response.iter_content():\n",
    "            handle.write(data)\n",
    "\n",
    "if not os.path.exists(location):\n",
    "    os.mkdir(location)\n",
    "     \n",
    "for item in data.values():\n",
    "    path = os.path.normpath(os.path.join(location, item['file']))\n",
    "\n",
    "    if os.path.exists(path):\n",
    "        print(\"Reusing locally cached:\", path)\n",
    "        \n",
    "    else:\n",
    "        print(\"Starting download:\", item['file'])\n",
    "        url = \"http://www.cntk.ai/jup/dat/DSSM/%s.csv\"%(item['file'])\n",
    "        print(url)\n",
    "        download(url, path)\n",
    "        print(\"Download completed\")\n",
    "    item['file'] = path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "original_source": [
     "### Reader\n",
     "\n",
     "We will be using the CTF deserializer to read the input data. However, one can write their own readers or use numpy arrays to provide data into CNTK modeling workflow. You may want to open the CTF files with a text editor to parse the input. Note, the CTF deserializer has the capability to scale across production scale data sizes spanning mulitple disks. The reader also abstracts the randomization of the large scale with a simple flag, an added convenience and time savings for the programmer. "
    ]
   },
   "source": [
    "_unchecked_\n",
    "\n",
    "### 读者\n",
    "\n",
    "我们将使用反来读取输入数据。但是, 可以编写自己的读取器或使用 numpy 数组为 CNTK 建模工作流提供数据。您可能希望使用文本编辑器打开 \"周大福文件\" 来分析输入。注意, 周大福反具有跨多个磁盘的生产规模数据大小的扩展能力。读者还可以用一个简单的标志来抽象出大规模的随机化, 为程序员增加了方便和节省时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the vocabulary size (QRY-stands for question and ANS stands for answer)\n",
    "QRY_SIZE = 1204\n",
    "ANS_SIZE = 1019\n",
    "\n",
    "def create_reader(path, is_training):\n",
    "    return C.io.MinibatchSource(C.io.CTFDeserializer(path, C.io.StreamDefs(\n",
    "         query = C.io.StreamDef(field='S0', shape=QRY_SIZE,  is_sparse=True),\n",
    "         answer  = C.io.StreamDef(field='S1', shape=ANS_SIZE, is_sparse=True)\n",
    "     )), randomize=is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\DSSM\\train.pair.tok.ctf\n",
      "data\\DSSM\\valid.pair.tok.ctf\n"
     ]
    }
   ],
   "source": [
    "train_file = data['train']['file']\n",
    "print(train_file)\n",
    "\n",
    "if os.path.exists(train_file):\n",
    "    train_source = create_reader(train_file, is_training=True)\n",
    "else:\n",
    "    raise ValueError(\"Cannot locate file {0} in current directory {1}\".format(train_file, os.getcwd()))\n",
    "\n",
    "validation_file = data['val']['file']\n",
    "print(validation_file)\n",
    "if os.path.exists(validation_file):\n",
    "    val_source = create_reader(validation_file, is_training=False)\n",
    "else:\n",
    "    raise ValueError(\"Cannot locate file {0} in current directory {1}\".format(validation_file, os.getcwd()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "original_source": [
     "## Model creation\n",
     "\n",
     "The proposed LSTM-RNN model sequentially takes each word in a sentence, extracts its information, and embeds it into a semantic vector. Due to its ability to capture long term memory, the LSTM-RNN accumulates increasingly richer information as it goes through the sentence, and when it reaches the last word, the hidden layer of the network provides a semantic representation of the whole sentence. The `last` block is then projected to a `query_vector` space, also referred to semantic feature in the figure above.\n",
     "\n",
     "\n",
     "                                                                        \"query vector\"\n",
     "                                                                              ^\n",
     "                                                                              |\n",
     "                                                                          +-------+  \n",
     "                                                                          | Dense |  \n",
     "                                                                          +-------+  \n",
     "                                                                              ^         \n",
     "                                                                              |         \n",
     "                                                                         +---------+  \n",
     "                                                                         | Dropout |  \n",
     "                                                                         +---------+\n",
     "                                                                              ^\n",
     "                                                                              |         \n",
     "                                                                          +-------+  \n",
     "                                                                          | Dense |  \n",
     "                                                                          +-------+  \n",
     "                                                                              ^         \n",
     "                                                                              |         \n",
     "                                                                          +------+   \n",
     "                                                                          | last |  \n",
     "                                                                          +------+  \n",
     "                                                                              ^  \n",
     "                                                                              |         \n",
     "                              +------+   +------+   +------+   +------+   +------+   \n",
     "                         0 -->| LSTM |-->| LSTM |-->| LSTM |-->| LSTM |-->| LSTM |\n",
     "                              +------+   +------+   +------+   +------+   +------+   \n",
     "                                  ^          ^          ^          ^          ^\n",
     "                                  |          |          |          |          |\n",
     "                              +-------+  +-------+  +-------+  +-------+  +-------+\n",
     "                              | Embed |  | Embed |  | Embed |  | Embed |  | Embed | \n",
     "                              +-------+  +-------+  +-------+  +-------+  +-------+\n",
     "                                  ^          ^          ^          ^          ^\n",
     "                                  |          |          |          |          |\n",
     "                    query  ------>+--------->+--------->+--------->+--------->+\n",
     "    \n",
     " \n",
     " Similarly we can project the answer sentence to `answer_vector`. However, before we create our model. Let us define the input variables for our model. Note, there is a query and paired with it there is an answer. Given both of these are a sequence of words we define "
    ]
   },
   "source": [
    "_unchecked_\n",
    "\n",
    "## 模型创建\n",
    "\n",
    "所提出的 LSTM-RNN 模型依次取句中的每个单词, 提取其信息, 并将其嵌入到一个语义向量中。由于它能够捕捉长期记忆, LSTM-RNN 积累了越来越丰富的信息, 因为它通过这个句子, 当它到达最后一个词, 网络的隐藏层提供了整个句子的语义表示。然后将 `last` block 投影到 `query_vector` 空间, 也称为上面图中的语义特征。\n",
    "\n",
    "    `                                                                    \"query vector\"\n",
    "                                                                              ^\n",
    "                                                                              |\n",
    "                                                                          +-------+  \n",
    "                                                                          | Dense |  \n",
    "                                                                          +-------+  \n",
    "                                                                              ^         \n",
    "                                                                              |         \n",
    "                                                                         +---------+  \n",
    "                                                                         | Dropout |  \n",
    "                                                                         +---------+\n",
    "                                                                              ^\n",
    "                                                                              |         \n",
    "                                                                          +-------+  \n",
    "                                                                          | Dense |  \n",
    "                                                                          +-------+  \n",
    "                                                                              ^         \n",
    "                                                                              |         \n",
    "                                                                          +------+   \n",
    "                                                                          | last |  \n",
    "                                                                          +------+  \n",
    "                                                                              ^  \n",
    "                                                                              |         \n",
    "                              +------+   +------+   +------+   +------+   +------+   \n",
    "                         0 -->| LSTM |-->| LSTM |-->| LSTM |-->| LSTM |-->| LSTM |\n",
    "                              +------+   +------+   +------+   +------+   +------+   \n",
    "                                  ^          ^          ^          ^          ^\n",
    "                                  |          |          |          |          |\n",
    "                              +-------+  +-------+  +-------+  +-------+  +-------+\n",
    "                              | Embed |  | Embed |  | Embed |  | Embed |  | Embed | \n",
    "                              +-------+  +-------+  +-------+  +-------+  +-------+\n",
    "                                  ^          ^          ^          ^          ^\n",
    "                                  |          |          |          |          |\n",
    "                    query  ------>+--------->+--------->+--------->+--------->+\n",
    "    `\n",
    "\n",
    "同样, 我们可以将应答句投射到 `answer_vector` 。但是, 在我们创建模型之前。让我们为我们的模型定义输入变量。注意, 有一个查询, 并与它配对有一个答案。考虑到这两个词, 我们定义了一系列的单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the containers for input feature (x) and the label (y)\n",
    "qry = C.sequence.input_variable(QRY_SIZE)\n",
    "ans = C.sequence.input_variable(ANS_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "original_source": [
     "**Notice**: Do you smell any problem with the aforementioned statements. If you want to see what would happen if you were to go with the declarations above, please comment out the 4 statements below and run the model. You will find that your model throws an exception. The details of the exception is explained [here](https://cntk.ai/pythondocs/Manual_How_to_debug.html#Runtime-errors).\n",
     "\n",
     "Each sequence in CNTK, is associated with a dynamic axis representing the number of words in the sequence. Intuitively, when you have sequences of different sizes and vocabularies, each of them need to have their own dynamic axis. This is facilitated by declaring the input data containers with a named axis. Strictly speaking you could name just one, the other one would be a default dynamic axis. However, for clarity we name the two axis separately."
    ]
   },
   "source": [
    "_unchecked_\n",
    "\n",
    "**注意**: 您是否闻到上述声明的任何问题。如果你想看看会发生什么, 如果你去与上述声明, 请注释出下面的4语句, 并运行模型。你会发现你的模型抛出一个异常。该异常的详细信息将在[此处](https://cntk.ai/pythondocs/Manual_How_to_debug.html#Runtime-errors)说明。\n",
    "\n",
    "CNTK 中的每个序列都与表示序列中的单词数的动态轴关联。直观地, 当你有不同大小和词汇的序列时, 它们都需要有自己的动态轴。通过使用命名轴声明输入数据容器来促进这一点。严格地说, 你可以只命名一个, 另一个将是一个默认的动态轴。但是, 为了清楚起见, 我们分别命名两个轴。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the containers for input feature (x) and the label (y)\n",
    "axis_qry = C.Axis.new_unique_dynamic_axis('axis_qry')\n",
    "qry = C.sequence.input_variable(QRY_SIZE, sequence_axis=axis_qry)\n",
    "\n",
    "axis_ans = C.Axis.new_unique_dynamic_axis('axis_ans')\n",
    "ans = C.sequence.input_variable(ANS_SIZE, sequence_axis=axis_ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "original_source": [
     "Before we can create the model we need to specify a few parameters associated with the network architecture."
    ]
   },
   "source": [
    "_unchecked_\n",
    "\n",
    "在我们可以创建模型之前, 我们需要指定一些与网络体系结构相关的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMB_DIM   = 25 # Embedding dimension\n",
    "HIDDEN_DIM = 50 # LSTM dimension\n",
    "DSSM_DIM = 25 # Dense layer dimension  \n",
    "NEGATIVE_SAMPLES = 5\n",
    "DROPOUT_RATIO = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(qry, ans):\n",
    "    with C.layers.default_options(initial_state=0.1):\n",
    "        qry_vector = C.layers.Sequential([\n",
    "            C.layers.Embedding(EMB_DIM, name='embed'),\n",
    "            C.layers.Recurrence(C.layers.LSTM(HIDDEN_DIM), go_backwards=False),\n",
    "            C.sequence.last,\n",
    "            C.layers.Dense(DSSM_DIM, activation=C.relu, name='q_proj'),\n",
    "            C.layers.Dropout(DROPOUT_RATIO, name='dropout qdo1'),\n",
    "            C.layers.Dense(DSSM_DIM, activation=C.tanh, name='q_enc')\n",
    "        ])\n",
    "        \n",
    "        ans_vector = C.layers.Sequential([\n",
    "            C.layers.Embedding(EMB_DIM, name='embed'),\n",
    "            C.layers.Recurrence(C.layers.LSTM(HIDDEN_DIM), go_backwards=False),\n",
    "            C.sequence.last,\n",
    "            C.layers.Dense(DSSM_DIM, activation=C.relu, name='a_proj'),\n",
    "            C.layers.Dropout(DROPOUT_RATIO, name='dropout ado1'),\n",
    "            C.layers.Dense(DSSM_DIM, activation=C.tanh, name='a_enc')\n",
    "        ])\n",
    "\n",
    "    return {\n",
    "        'query_vector': qry_vector(qry),\n",
    "        'answer_vector': ans_vector(ans)\n",
    "    }\n",
    "\n",
    "# Create the model and store reference in `network` dictionary\n",
    "network = create_model(qry, ans)\n",
    "\n",
    "network['query'], network['axis_qry'] = qry, axis_qry\n",
    "network['answer'], network['axis_ans'] = ans, axis_ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "original_source": [
     "## Training\n",
     "\n",
     "Now that we have created a network, the next step is to find a suitable loss function where if a `question` is paired with the correct `answer`, the loss would be 0 else it would be 1. In other words, this loss should maximize the similarity (dot  product) between the answer vector which appears close to the answer vector  and minimize the similarity of between the answer and question vector that do not answer each other.  \n",
     "\n",
     "The use cases of DSSM often appear in information retrieval where for a given query or question there are few answers amongst an ocean of poor or non-answers. The input data as in this case is a pair of query and answer (document or advertisement) that attracted a click. A classical way to train would be a a binary classifier to predict click / no-click (or equivalently a 2-class classifier - one class each for click or no click). One could generate pairs of query and incorrect answers (as no-click data). However, one way to simulate no-click data is to use query and answers for other queries within a minibatch. This is the concept behind `cosine_distance_with_negative_samples` function. Note: This function returns 1 for correct the question and answer pair and 0 for incorrect, which is referred to as *similarity*. Hence, we use 1- `cosine_distance_with_negative_samples` as our loss function."
    ]
   },
   "source": [
    "_unchecked_\n",
    "\n",
    "## 培训\n",
    "\n",
    "现在我们已经创建了一个网络, 下一步是找到一个合适的损失函数, 如果 `question` 与正确的配对 `answer` , 则损失将是 0, 否则将是1。换言之, 这一损失应最大限度的相似性 (点产品) 之间的答案向量, 似乎接近回答向量和最小的相似性之间的答案和问题的向量, 不回答对方。\n",
    "\n",
    "DSSM 的用例通常出现在信息检索中, 对于给定的查询或问题, 在一个贫穷或非答案的海洋中很少有答案。在这种情况下, 输入数据是一对查询和应答 (文档或广告), 它吸引了单击。一个经典的方式来训练将是一个二进制分类器预测点击/不点击 (或等效的2类分类器-一个类, 每个点击或不点击)。可以生成对查询和不正确的答案 (如无单击数据)。但是, 模拟无单击数据的一种方法是对 minibatch 中的其他查询使用查询和答案。这是 `cosine_distance_with_negative_samples` 函数背后的概念。注意: 此函数返回1以更正问题和应答对, 0 为不正确, 这称为*相似性*。因此, 我们使用 1- `cosine_distance_with_negative_samples` 作为我们的损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_loss(vector_a, vector_b):\n",
    "    qry_ans_similarity = C.cosine_distance_with_negative_samples(vector_a, \\\n",
    "                                                                 vector_b, \\\n",
    "                                                                 shift=1, \\\n",
    "                                                                 num_negative_samples=5)\n",
    "    return 1 - qry_ans_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "MAX_EPOCHS = 5\n",
    "EPOCH_SIZE = 10000\n",
    "MINIBATCH_SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "def create_trainer(reader, network):\n",
    "    \n",
    "    # Setup the progress updater\n",
    "    progress_writer = C.logging.ProgressPrinter(tag='Training', num_epochs=MAX_EPOCHS)\n",
    "\n",
    "    # Set learning parameters\n",
    "    lr_per_sample     = [0.0015625]*20 + \\\n",
    "                        [0.00046875]*20 + \\\n",
    "                        [0.00015625]*20 + \\\n",
    "                        [0.000046875]*10 + \\\n",
    "                        [0.000015625]\n",
    "    lr_schedule       = C.learning_parameter_schedule_per_sample(lr_per_sample, \\\n",
    "                                                 epoch_size=EPOCH_SIZE)\n",
    "    mms               = [0]*20 + [0.9200444146293233]*20 + [0.9591894571091382]\n",
    "    mm_schedule       = C.learners.momentum_schedule(mms, \\\n",
    "                                                     epoch_size=EPOCH_SIZE, \\\n",
    "                                                     minibatch_size=MINIBATCH_SIZE)\n",
    "    l2_reg_weight     = 0.0002\n",
    "\n",
    "    model = C.combine(network['query_vector'], network['answer_vector'])\n",
    "\n",
    "    #Notify the network that the two dynamic axes are indeed same\n",
    "    query_reconciled = C.reconcile_dynamic_axes(network['query_vector'], network['answer_vector'])\n",
    "  \n",
    "    network['loss'] = create_loss(query_reconciled, network['answer_vector'])\n",
    "    network['error'] = None\n",
    "\n",
    "    print('Using momentum sgd with no l2')\n",
    "    dssm_learner = C.learners.momentum_sgd(model.parameters, lr_schedule, mm_schedule)\n",
    "\n",
    "    network['learner'] = dssm_learner\n",
    " \n",
    "    print('Using local learner')\n",
    "    # Create trainer\n",
    "    return C.Trainer(model, (network['loss'], network['error']), network['learner'], progress_writer)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using momentum sgd with no l2\n",
      "Using local learner\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the trainer\n",
    "trainer = create_trainer(train_source, network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train \n",
    "def do_train(network, trainer, train_source):\n",
    "    # define mapping from intput streams to network inputs\n",
    "    input_map = {\n",
    "        network['query']: train_source.streams.query,\n",
    "        network['answer']: train_source.streams.answer\n",
    "        } \n",
    "\n",
    "    t = 0\n",
    "    for epoch in range(MAX_EPOCHS):         # loop over epochs\n",
    "        epoch_end = (epoch+1) * EPOCH_SIZE\n",
    "        while t < epoch_end:                # loop over minibatches on the epoch\n",
    "            data = train_source.next_minibatch(MINIBATCH_SIZE, input_map= input_map)  # fetch minibatch\n",
    "            trainer.train_minibatch(data)               # update model with it\n",
    "            t += MINIBATCH_SIZE\n",
    "\n",
    "        trainer.summarize_training_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate per 1 samples: 0.0015625\n",
      "Momentum per 1 samples: 0.0\n",
      "Finished Epoch[1 of 5]: [Training] loss = 0.343046 * 1522, metric = 0.00% * 1522 5.720s (266.1 samples/s);\n",
      "Finished Epoch[2 of 5]: [Training] loss = 0.102804 * 1530, metric = 0.00% * 1530 3.464s (441.7 samples/s);\n",
      "Finished Epoch[3 of 5]: [Training] loss = 0.066461 * 1525, metric = 0.00% * 1525 3.402s (448.3 samples/s);\n",
      "Finished Epoch[4 of 5]: [Training] loss = 0.048511 * 1534, metric = 0.00% * 1534 3.390s (452.5 samples/s);\n",
      "Finished Epoch[5 of 5]: [Training] loss = 0.035384 * 1510, metric = 0.00% * 1510 3.383s (446.3 samples/s);\n"
     ]
    }
   ],
   "source": [
    "do_train(network, trainer, train_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "original_source": [
     "## Validate\n",
     "\n",
     "Once the model is trained we want to select a model that has similar error with the validation (hold-out set) as the error with the training set. \n",
     "\n",
     "**Suggested Activity**: Vary the number of epochs and check the training and the validation error.\n",
     "\n",
     "The chosen model would then be used for prediction. "
    ]
   },
   "source": [
    "_unchecked_\n",
    "\n",
    "## 验证\n",
    "\n",
    "一旦模型被训练, 我们要选择一个模型, 它与验证 (\"保持\" 集) 具有类似的错误, 作为训练集的错误。\n",
    "\n",
    "**建议的活动**: 更改世纪数并检查培训和验证错误。\n",
    "\n",
    "然后, 选择的模型将用于预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Validate\n",
    "def do_validate(network, val_source):\n",
    "    # process minibatches and perform evaluation\n",
    "    progress_printer = C.logging.ProgressPrinter(tag='Evaluation', num_epochs=0)\n",
    "\n",
    "    val_map = {\n",
    "        network['query']: val_source.streams.query,\n",
    "        network['answer']: val_source.streams.answer\n",
    "        } \n",
    "\n",
    "    evaluator = C.eval.Evaluator(network['loss'], progress_printer)\n",
    "\n",
    "    while True:\n",
    "        minibatch_size = 100\n",
    "        data = val_source.next_minibatch(minibatch_size, input_map=val_map)\n",
    "        if not data:                                 # until we hit the end\n",
    "            break\n",
    "\n",
    "        evaluator.test_minibatch(data)\n",
    "\n",
    "    evaluator.summarize_test_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Evaluation [1]: Minibatch[1-35]: metric = 0.02% * 410;\n"
     ]
    }
   ],
   "source": [
    "do_validate(network, val_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "original_source": [
     "## Predict\n",
     "\n",
     "We will now create a vector representation of the query and the answer. Then compute the cosine similarity between the two vectors. When the answer is close to the question one would get a high similarity, while an incorrect / partially relevant question / answer pair would result in a smaller similarity. These scores are often used for ranking web documents in response to a query."
    ]
   },
   "source": [
    "_unchecked_\n",
    "\n",
    "## 预测\n",
    "\n",
    "现在, 我们将创建一个向量表示的查询和答案。然后计算两个向量之间的余弦相似度。当答案接近这个问题时, 你会得到一个很高的相似性, 而一个不正确/部分相关的问题/答案对会导致更小的相似性。这些分数通常用于对 web 文档进行排序以响应查询。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Indices: [1202, 1154, 267, 321, 357, 648, 1070, 905, 549, 6, 1203]\n",
      "Answer Indices: [1017, 135, 91, 137, 1018]\n",
      "Poor Answer Indices: [1017, 501, 452, 533, 1018]\n"
     ]
    }
   ],
   "source": [
    "# load dictionaries\n",
    "query_wl = [line.rstrip('\\n') for line in open(data['query']['file'])]\n",
    "answers_wl = [line.rstrip('\\n') for line in open(data['answer']['file'])]\n",
    "query_dict = {query_wl[i]:i for i in range(len(query_wl))}\n",
    "answers_dict = {answers_wl[i]:i for i in range(len(answers_wl))}\n",
    "\n",
    "# let's run a sequence through\n",
    "qry = 'BOS what contribution did  e1  made to science in 1665 EOS'\n",
    "ans = 'BOS book author book_editions_published EOS'\n",
    "ans_poor = 'BOS language human_language main_country EOS'\n",
    "\n",
    "qry_idx = [query_dict[w+' '] for w in qry.split()] # convert to query word indices\n",
    "print('Query Indices:', qry_idx)\n",
    "\n",
    "ans_idx = [answers_dict[w+' '] for w in ans.split()] # convert to answer word indices\n",
    "print('Answer Indices:', ans_idx)\n",
    "\n",
    "ans_poor_idx = [answers_dict[w+' '] for w in ans_poor.split()] # convert to fake answer word indices\n",
    "print('Poor Answer Indices:', ans_poor_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "original_source": [
     "Convert the query, answer and the fake answer to one-hot representation. This is a necessary step since the input to our trained network takes one-hot encoded input. "
    ]
   },
   "source": [
    "_unchecked_\n",
    "\n",
    "将查询、答案和假答案转换为一个热表示。这是一个必要的步骤, 因为输入到我们训练有素的网络接受一热编码输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the one hot representations\n",
    "qry_onehot = np.zeros([len(qry_idx),len(query_dict)], np.float32)\n",
    "for t in range(len(qry_idx)):\n",
    "    qry_onehot[t,qry_idx[t]] = 1\n",
    "    \n",
    "ans_onehot = np.zeros([len(ans_idx),len(answers_dict)], np.float32)\n",
    "for t in range(len(ans_idx)):\n",
    "    ans_onehot[t,ans_idx[t]] = 1\n",
    "    \n",
    "ans_poor_onehot = np.zeros([len(ans_poor_idx),len(answers_dict)], np.float32)\n",
    "for t in range(len(ans_poor_idx)):\n",
    "    ans_poor_onehot[t, ans_poor_idx[t]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "original_source": [
     "For each of the query and the answer one-hot encoded input, create the embeddings. Note: we use the answer embedding for both the correct answer and the poor answer. We compute the cosine similarity between the query and answer pair. The relative value of the cosine similarity with a higher value indicating a better answer."
    ]
   },
   "source": [
    "_unchecked_\n",
    "\n",
    "对于每个查询和答案一热编码输入, 创建嵌入。注意: 我们将答案嵌入到正确答案和拙劣答案中。我们计算了查询和应答对之间的余弦相似性。余弦相似性的相对值与更高的值表示更好的答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query to Answer similarity: 0.99995367043\n",
      "Query to poor-answer similarity: 0.999941420215\n"
     ]
    }
   ],
   "source": [
    "qry_embedding = network['query_vector'].eval([qry_onehot])\n",
    "ans_embedding = network['answer_vector'].eval([ans_onehot])\n",
    "ans_poor_embedding = network['answer_vector'].eval([ans_poor_onehot])\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "print('Query to Answer similarity:', 1-cosine(qry_embedding, ans_embedding))\n",
    "print('Query to poor-answer similarity:', 1-cosine(qry_embedding, ans_poor_embedding))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}